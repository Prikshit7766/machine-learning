{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0384cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                           names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e95cc7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbec15b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022f241e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aeb8c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please don't text me anymore. I have nothing else to say.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[\"message\"].loc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e51a9",
   "metadata": {},
   "source": [
    "#### data cleaning and preprocessing\n",
    "1 Tokenization , stopwords , stemming , lemmatization  \n",
    "2 text to vectors  -> BOW , TF-Idf , word2vec . avgword2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f44ba917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f7c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps = PorterStemmer ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4acbe2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "265d089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though',\n",
       " 'freemsg hey darl week word back like fun still tb ok xxx std chg send rcv',\n",
       " 'even brother like speak treat like aid patent',\n",
       " 'per request mell mell oru minnaminungint nurungu vettam set callertun caller press copi friend callertun',\n",
       " 'winner valu network custom select receivea prize reward claim call claim code kl valid hour',\n",
       " 'mobil month u r entitl updat latest colour mobil camera free call mobil updat co free']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6ca074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "y=pd.get_dummies(messages['label'])\n",
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90e4de",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d80f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e3be6",
   "metadata": {},
   "source": [
    "# Creating the Bag of Words model + MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "376c7d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500,binary=True)\n",
    "X_train_BOW= cv.fit_transform(X_train).toarray()\n",
    "X_test_BOW= cv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0f8873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1404fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0eecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train_BOW, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57dd3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred1=spam_detect_model.predict(X_test_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbdfc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9838565022421525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "score=accuracy_score(y_test,y_pred1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f734f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       959\n",
      "           1       0.93      0.96      0.94       156\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.96      0.97      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4afbb",
   "metadata": {},
   "source": [
    "# Creating the TFIDF model +MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "270b22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the TFIDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=2500)\n",
    "X_train_TFIDF= tv.fit_transform(X_train).toarray()\n",
    "X_test_TFIDF= tv.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e4f7d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_TFIDF[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ee0289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(X_train_TFIDF, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c84a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "y_pred2=spam_detect_model.predict(X_test_TFIDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d40396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766816143497757\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,y_pred2)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba56442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       981\n",
      "           1       0.84      1.00      0.91       134\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.92      0.99      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a9115",
   "metadata": {},
   "source": [
    "# Creating the TFIDF model +  random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b11211b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train_TFIDF,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3b36773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9829596412556054\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "y_pred3=classifier.predict(X_test_TFIDF)\n",
    "score=accuracy_score(y_test,y_pred3)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa2fc362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       974\n",
      "           1       0.88      1.00      0.94       141\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.94      0.99      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred3,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af6363",
   "metadata": {},
   "source": [
    "# Creating the Bag of Words model + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd12cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier()\n",
    "classifier.fit(X_train_BOW,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6775aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9829596412556054\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "y_pred4=classifier.predict(X_test_BOW)\n",
    "score=accuracy_score(y_test,y_pred4)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8a3de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       974\n",
      "           1       0.88      1.00      0.94       141\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.94      0.99      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred4,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15e768",
   "metadata": {},
   "source": [
    "## Word2vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "451e35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3aac6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "630db654",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22df2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c27e4a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f253aad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazy available bugis n great world la e buffet cine got amore wat'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "233b3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for sent in corpus:\n",
    "    sent_token=sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent)) #simple_preprocess Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5a78735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b8aedb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5564"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90a6aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, '', 'What you doing?how are you?'],\n",
       " [0, '', 'Where @'],\n",
       " [0, '', '645'],\n",
       " [0, '', 'Can a not?'],\n",
       " [0, '', ':) '],\n",
       " [0, '', 'What you doing?how are you?'],\n",
       " [0, '', ':( but your not here....'],\n",
       " [0, '', ':-) :-)']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i,j,k] for i,j,k in zip(list(map(len,corpus)),corpus, messages['message']) if i<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f7822",
   "metadata": {},
   "source": [
    "Because of this reason, 'simple_preprocess' ignores these rows in the corpus and the final dimensions of X and y do not match i.e 5564(X) vs 5572(y). Therefore train_test_split also throws an error.\n",
    "\n",
    "To fix this, recalculate y by removing the messages dataframe rows corresponding to the above blank sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dd6e75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5564,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = messages[list(map(lambda x: len(x)>0 ,corpus))]\n",
    "y=pd.get_dummies(y['label'])\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24c21356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making word2vec from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6bf6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9758919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets train Word2vec from scratch\n",
    "model=gensim.models.Word2Vec(words,window=5,vector_size=100,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40ab9c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call', 'get', 'ur', 'gt', 'lt', 'go', 'ok', 'day', 'free', 'know']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9124d955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5564"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "086d749a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03f9f3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('work', 0.9969403743743896),\n",
       " ('much', 0.996876060962677),\n",
       " ('money', 0.9968047738075256),\n",
       " ('went', 0.996778130531311),\n",
       " ('sent', 0.9967295527458191),\n",
       " ('oh', 0.9967256784439087),\n",
       " ('really', 0.9967249631881714),\n",
       " ('going', 0.9967221617698669),\n",
       " ('like', 0.996698260307312),\n",
       " ('would', 0.9966953992843628)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('kid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f84e359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('live', 0.9990087151527405),\n",
       " ('ur', 0.9989923238754272),\n",
       " ('go', 0.9989807605743408),\n",
       " ('min', 0.9989736080169678),\n",
       " ('txt', 0.9989727735519409),\n",
       " ('best', 0.9989722967147827),\n",
       " ('msg', 0.9989525079727173),\n",
       " ('next', 0.9989442825317383),\n",
       " ('back', 0.9989382028579712),\n",
       " ('give', 0.9989266991615295)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f28a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('year', 0.9994580149650574),\n",
       " ('hello', 0.9993191361427307),\n",
       " ('day', 0.9993175864219666),\n",
       " ('make', 0.9993124008178711),\n",
       " ('like', 0.9992846846580505),\n",
       " ('new', 0.999279797077179),\n",
       " ('dear', 0.9992725849151611),\n",
       " ('dont', 0.9992671012878418),\n",
       " ('keep', 0.9992635846138),\n",
       " ('money', 0.999251663684845)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a5b4ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['happy'].shape  # dimension of single word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b62f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to apply avg word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d544f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    #sent = [word for word in doc if word in model.wv.index_to_key]\n",
    "    #print(sent)\n",
    "    \n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv.index_to_key],axis=0)\n",
    "                #or [np.zeros(len(model.wv.index_to_key))], axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea1def4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\all\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\all\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d70bce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d989aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['performed']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[73]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09a0703e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc7bf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b373a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/5564 [00:00<?, ?it/s]C:\\Users\\HP\\anaconda3\\envs\\all\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\HP\\anaconda3\\envs\\all\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5564/5564 [00:00<00:00, 6663.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words 5564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#apply for the entire sentences\n",
    "X=[]\n",
    "count=0\n",
    "for i in tqdm(range(len(words))):\n",
    "    count=count+1\n",
    "    X.append(avg_word2vec(words[i]))\n",
    "    \n",
    "print (\"total words\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a3a8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e493195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\all\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_new=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5316a754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10532334,  0.29218706,  0.14601143,  0.02279398,  0.03180818,\n",
       "       -0.32373455,  0.08819105,  0.5146341 , -0.16565207, -0.15271027,\n",
       "       -0.15452074, -0.3383404 , -0.00425214,  0.095595  ,  0.07988171,\n",
       "       -0.27909568,  0.01902971, -0.33567894, -0.01489698, -0.44326127,\n",
       "        0.07172848,  0.15127851,  0.10022926, -0.12536488, -0.10606787,\n",
       "        0.02655315, -0.21024317, -0.14019944, -0.21742932,  0.05002345,\n",
       "        0.29315338,  0.05448255,  0.15369211, -0.23121105, -0.12070017,\n",
       "        0.26239416,  0.03629923, -0.19900161, -0.17211859, -0.37857762,\n",
       "        0.06459386, -0.23299868, -0.08966768,  0.04602126,  0.2470609 ,\n",
       "       -0.13115536, -0.17995109,  0.03126285,  0.14228906,  0.2386996 ,\n",
       "        0.16908745, -0.27767193, -0.0568573 , -0.02133485, -0.12629987,\n",
       "        0.20706278,  0.17147042, -0.03659447, -0.267844  ,  0.06406327,\n",
       "        0.10153774,  0.1179468 , -0.12325127,  0.00103988, -0.2671925 ,\n",
       "        0.1614449 ,  0.08290652,  0.19251798, -0.28059915,  0.32709584,\n",
       "       -0.16534577,  0.12583238,  0.30866483, -0.0747666 ,  0.28442994,\n",
       "        0.18123026, -0.01232522, -0.09114334, -0.19835365,  0.1280522 ,\n",
       "       -0.07927089,  0.00493502, -0.22887607,  0.34722862, -0.04897973,\n",
       "       -0.00126284, -0.12354526,  0.33168188,  0.34898993,  0.0817267 ,\n",
       "        0.35019442,  0.14123127,  0.05306219,  0.13632405,  0.33540884,\n",
       "        0.27271155,  0.11399945, -0.29091826,  0.10417092, -0.04835483],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215041e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6eeb37c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c860514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb1afaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5564,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d1a5cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5564,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
